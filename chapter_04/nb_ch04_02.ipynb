{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbTYFRhJoaBu"
   },
   "source": [
    "# Calculation of the cross entropy loss (NLL) for a classification tasks\n",
    "\n",
    "\n",
    "**Goal:** In this notebook you will use Keras to set up a CNN for classification of MNIST images and calculate the cross entropy before the CNN was trained. You will first calculate the cross entropy loss for a binary classification problem and then for a classification problem with ten classes. You will use basic numpy functions to calculate the loss that is expected from random guessing and see that an untrained CNN is not better than guessing.\n",
    "\n",
    "**Usage:** Before working through this notebook we recommend to read chapter 4.2. The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
    "\n",
    "**Dataset:** You work with the MNIST dataset. You have 60'000 28x28 pixel greyscale images of digits (0-9).\n",
    "\n",
    "**Content:**\n",
    "* load the original MNIST data \n",
    "* create a subset the of the data to make it binary classification problem\n",
    "* define a CNN in Keras\n",
    "* evaluation of the cross entropy loss function of the untrained CNN for only two classes\n",
    "* evaluation of the cross entropy loss function of the untrained CNN for all classes\n",
    "* implement the loss function yourself using the predicted probabilities and numpy\n",
    "\n",
    "\n",
    "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_04/nb_ch04_02.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEIS4WvpsT5t"
   },
   "source": [
    "#### Imports\n",
    "\n",
    "First you load all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6S_hQX5oaBw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h_3TS0CtJJb"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Loading and preparing the MNIST data\n",
    "You download the MNIST data, normalize the pixel-values to be between 0 and 1 and create  a sub-dataset which only contains images with the labels 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4sZ8lqFfoaB2",
    "outputId": "2229412b-8bf6-4302-c6e0-b5056b976ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 28, 28, 1), (12665, 2), (12665, 28, 28, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train=x_train / 255 #divide by 255 so that they are in range 0 to 1\n",
    "X_train=np.reshape(X_train, (X_train.shape[0],28,28,1))\n",
    "Y_train=tensorflow.keras.utils.to_categorical(y_train,10) # one-hot encoding\n",
    "\n",
    "# define sub data containing only images with 0 or 1\n",
    "idx = (y_train==0)|(y_train==1)\n",
    "\n",
    "X_train_sub = X_train[idx]\n",
    "Y_train_sub = y_train[idx]\n",
    "Y_train_sub=tensorflow.keras.utils.to_categorical(Y_train_sub,2) # one-hot encoding\n",
    "\n",
    "Y_train.shape, X_train.shape, Y_train_sub.shape, X_train_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaRFUEP8HJkq"
   },
   "source": [
    "## CNN model\n",
    "\n",
    "You use the same CNN model as in chapter 2. First you will use it to evaluate the loss for only two classes (0 and 1)  and then for all ten classes of the MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSfYQ4f1KYVp"
   },
   "outputs": [],
   "source": [
    "# here you define hyperparameter of the CNN\n",
    "batch_size = 128\n",
    "nb_classes = 2  # for the sub data you only have 2 classes\n",
    "img_rows, img_cols = 28, 28\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xwh0iYrHJk_"
   },
   "outputs": [],
   "source": [
    "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile model and intitialize weights\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "blxHZguwHJlG",
    "outputId": "7f15af6a-1f4b-44a1-d36d-7c9786e866dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 82        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 35,634\n",
      "Trainable params: 35,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize model along with number of model weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfNsOlQdYeOU"
   },
   "source": [
    "### Exercise 1: Evaluation of the untrained model using Keras\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "\n",
    "*Exercise 1: Compute the cross entropy loss with Keras and compare the result with the value that you would expect when you have a classification problem with two classes. Remember that the network is untrained and the predictions are basically just random guesses.*  \n",
    "\n",
    "You best use the function model.evaluate(), to get the cross entropy loss for the untrained network. The input for this function are the images and the corresponding true labels. The function returns the cross entropy loss and the accuracy of the predictions. Note that you use the sub dataset with only two classes (0 and 1). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2vxdCRSyOaz"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJMiyTTWyQka"
   },
   "source": [
    "Scroll down to see the solution.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "y_aIMKouYMjH",
    "outputId": "c9e33037-5717-4ecd-b328-4bc01ca3cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - 2s 167us/sample - loss: 0.6939 - accuracy: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6939027951350101, 0.46095538]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_sub, Y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZvyZHXFw9aM"
   },
   "source": [
    "If you have no idea about the training dataset,  you would expect each class with equal probability and your guess for every image would be 1/nr_of_classes. In the case of 2 classes, you predicit every image with a probability of around 0.5. The resulting cross-entropy, which is the negative log likelihood, is calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4wm5gZOPsl2R",
    "outputId": "f8fe4b7f-afc0-4cff-fce2-3e56fa2e8992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes=2\n",
    "-np.log(1/nr_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HkiaEquwdRF"
   },
   "source": [
    "### Return to the book \n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/Page_turn_icon_A.png\" width=\"120\" align=\"left\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcoPox0YIWXj"
   },
   "source": [
    "Let's now work with the full MNIST dataset (all ten classes). You can use the same network architecture as before, the only thing you need to change is the number of output nodes. You use 10 output nodes, one for each class (0, 1,..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0q16a2uIBkg"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile model and intitialize weights\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "dWx9gqJ6IUpZ",
    "outputId": "896cd292-ff29-4c99-e54c-63410738ba48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 35,962\n",
      "Trainable params: 35,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize model along with number of model weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-mY30BBI4LJ"
   },
   "source": [
    "Here you predict the probabilities for all images in the training data set. You did not train the network yet, therefore the probabilities will be around 10% for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "430DSTDIHJlP"
   },
   "outputs": [],
   "source": [
    "# Calculate the probailities for the training data\n",
    "Pred_prob = model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "TTyxe7xMJUKC",
    "outputId": "4653b444-ab70-4394-bf6c-1be2085e4b30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10749492, 0.10472608, 0.09103097, 0.09943033, 0.09795263,\n",
       "        0.09219333, 0.10371377, 0.10640517, 0.09195897, 0.10509381],\n",
       "       [0.10253069, 0.10584503, 0.08830696, 0.09874611, 0.09855303,\n",
       "        0.09201857, 0.1070503 , 0.10808311, 0.09251422, 0.10635199],\n",
       "       [0.10216265, 0.10498054, 0.09664252, 0.10318816, 0.09712867,\n",
       "        0.09183189, 0.10233367, 0.10906745, 0.09367013, 0.09899428],\n",
       "       [0.10977097, 0.09786078, 0.09714107, 0.09702702, 0.09982375,\n",
       "        0.09594168, 0.10972438, 0.09987551, 0.09232695, 0.10050784],\n",
       "       [0.10402357, 0.10470287, 0.09333142, 0.09796316, 0.09934756,\n",
       "        0.0928975 , 0.1069885 , 0.10527403, 0.09193941, 0.10353192]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_prob[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XcqY_UbNYyP2",
    "outputId": "46240007-f6a6-4054-87b5-43576e563ce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_prob.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9W46_Euob-ux"
   },
   "source": [
    "### Exercise 2: Calculate the loss function using numpy\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "*Exercise 2: Use numpy to calculate the value of the negative log-likelihood loss (=cross entropy) that you expect for the untrained CNN, which you have constructed above to discriminate between the 10 classes. Determine the cross entropy that results from the predicted probabilities (Pred_prob). To determine the cross entropy of the prediction, you can loop over each example and use its true label (Y_train) and the predicted probability for the true class. Do you get the cross entropy value that you have expected?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-nSWXYadTft"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hv1IEA74dPF6"
   },
   "source": [
    "Scroll down to see the solution.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM1EOk9WLkeh"
   },
   "source": [
    "In the next cell you calculate the cross entropy loss of each single image, then you sum up all individual losses and divide the sum with the nr of training examples. You take the negative of this result to get the NLL, also known as categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v9GkdLKcY5OU",
    "outputId": "706ab0a1-31e5-4b80-bdbb-c6fc053dc993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3108919594844184"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=np.zeros(len(X_train))\n",
    "Y=np.argmax(Y_train,axis=1)\n",
    "for i in range(0,len(X_train)):\n",
    "  loss[i]=np.log(Pred_prob[i][Y[i]])\n",
    "-np.sum(loss)/len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxOvZwJiMZDg"
   },
   "source": [
    "If you have no idea about the training dataset, your guess for every image would be 1/nr_of_classes, in the case with 10 classes, you would predicit every image with a probability around 0.1. The corresponding NLL is calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oWml2J8MKqwQ",
    "outputId": "5ee65010-b73e-4cf9-db34-67fb077fde6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes=10\n",
    "-np.log(1/nr_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJElA61ZMeZM"
   },
   "source": [
    "You get more or less the same result as as you got with the model.evaluate function for the untrained CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "n60Ql16SLZac",
    "outputId": "afa7e31e-eef7-411a-85df-b6fa627ea924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 9s 151us/sample - loss: 2.3109 - accuracy: 0.0827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.310891961542765, 0.08268333]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nb_ch04_02.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
